# Intro

Transformers are a type of neural network architecture that is specifically designed for natural language processing (NLP) tasks. The basic building block of a transformer is called the "transformer block," which consists of several layers of multi-headed self-attention and feedforward neural networks.

Here is a brief overview of the structure of a transformer:

1. Input Embedding: The input to a transformer is typically a sequence of tokens (e.g., words or subwords) that make up a sentence or document. Each token is first converted into a dense vector representation, known as an embedding.

2. Encoder: The encoder is a stack of transformer blocks that process the input sequence in a sequential manner. Each transformer block consists of two sub-layers:

   - Multi-Headed Self-Attention: This layer computes self-attention scores between all pairs of tokens in the input sequence. The self-attention scores represent the importance of each token in the context of the other tokens in the sequence.

   - Feedforward Neural Network: This layer applies a feedforward neural network to each token's embedding, using the self-attention scores to weight the contributions of other tokens in the sequence.

3. Decoder: The decoder is another stack of transformer blocks that takes as input the output of the encoder, along with the target sequence (i.e., the sequence that the model is trying to generate). The decoder is similar in structure to the encoder, but includes an additional masked self-attention layer that prevents the model from attending to future tokens in the target sequence.

4. Output Layer: The output layer is a linear layer that takes as input the final hidden states of the decoder, and produces a probability distribution over the possible output tokens (e.g., words or subwords).

Overall, the transformer architecture is designed to capture long-range dependencies and contextual information in natural language sequences, making it a powerful tool for a wide range of NLP tasks, including language modeling, machine translation, and text classification.

# Wikipedia article

A **transformer** is a [deep learning](https://en.wikipedia.org/wiki/Deep_learning "Deep learning") model that adopts the mechanism of [self-attention](https://en.wikipedia.org/wiki/Attention_(machine_learning) "Attention (machine learning)"), differentially weighting the significance of each part of the input data. It is used primarily in the fields of [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing "Natural language processing") (NLP)<sup id="cite_ref-:0_1-0"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-:0-1">[1]</a></sup> and [computer vision](https://en.wikipedia.org/wiki/Computer_vision "Computer vision") (CV).<sup id="cite_ref-2"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-2">[2]</a></sup>

Like [recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_networks "Recurrent neural networks") (RNNs), transformers are designed to process sequential input data, such as natural language, with applications towards tasks such as [translation](https://en.wikipedia.org/wiki/Statistical_machine_translation "Statistical machine translation") and [text summarization](https://en.wikipedia.org/wiki/Automatic_summarization "Automatic summarization"). However, unlike RNNs, transformers process the entire input all at once. The [attention mechanism](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#Self-attention) provides context for any position in the input sequence. For example, if the input data is a natural language sentence, the transformer does not have to process one word at a time. This allows for more [parallelization](https://en.wikipedia.org/wiki/Parallel_computing "Parallel computing") than RNNs and therefore reduces training times.<sup id="cite_ref-:0_1-1"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-:0-1">[1]</a></sup>

Transformers were introduced in 2017 by a team at [Google Brain](https://en.wikipedia.org/wiki/Google_Brain "Google Brain")<sup id="cite_ref-:0_1-2"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-:0-1">[1]</a></sup> and are increasingly the model of choice for NLP problems,<sup id="cite_ref-wolf2020_3-0"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-wolf2020-3">[3]</a></sup> replacing RNN models such as [long short-term memory](https://en.wikipedia.org/wiki/Long_short-term_memory "Long short-term memory") (LSTM). The additional training parallelization allows training on larger datasets. This led to the development of [pretrained systems](https://en.wikipedia.org/wiki/Transfer_learning "Transfer learning") such as [BERT](https://en.wikipedia.org/wiki/BERT_(language_model) "BERT (language model)") (Bidirectional Encoder Representations from Transformers) and [GPT](https://en.wikipedia.org/wiki/OpenAI#Generative_models "OpenAI") (Generative Pre-trained Transformer), which were trained with large language datasets, such as the [Wikipedia](https://en.wikipedia.org/wiki/Wikipedia "Wikipedia") Corpus and [Common Crawl](https://en.wikipedia.org/wiki/Common_Crawl "Common Crawl"), and can be fine-tuned for specific tasks.<sup id="cite_ref-:6_4-0"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-:6-4">[4]</a></sup><sup id="cite_ref-:7_5-0"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-:7-5">[5]</a></sup>

## Background\[[edit](https://en.wikipedia.org/w/index.php?title=Transformer_(machine_learning_model)&action=edit&section=1 "Edit section: Background")\]

Before transformers, most state-of-the-art NLP systems relied on gated RNNs, such as [LSTMs](https://en.wikipedia.org/wiki/Long_short-term_memory "Long short-term memory") and [gated recurrent units](https://en.wikipedia.org/wiki/Gated_recurrent_unit "Gated recurrent unit") (GRUs), with added [attention mechanisms](https://en.wikipedia.org/wiki/Attention_mechanism "Attention mechanism"). Transformers also make use of attention mechanisms but, unlike RNNs, do not have a recurrent structure. This means that provided with enough training data, attention mechanisms alone can match the performance of RNNs with attention.<sup id="cite_ref-:0_1-3"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-:0-1">[1]</a></sup>

### Sequential processing\[[edit](https://en.wikipedia.org/w/index.php?title=Transformer_(machine_learning_model)&action=edit&section=2 "Edit section: Sequential processing")\]

Gated RNNs process tokens sequentially, maintaining a state vector that contains a representation of the data seen prior to the current token. To process the ![{\textstyle n}](https://wikimedia.org/api/rest_v1/media/math/render/svg/cc6e1f880981346a604257ebcacdef24c0aca2d6)th token, the model combines the state representing the sentence up to token ![{\textstyle n-1}](https://wikimedia.org/api/rest_v1/media/math/render/svg/960c88fa1831b7505d9672de66058532fa5d4053) with the information of the new token to create a new state, representing the sentence up to token ![{\textstyle n}](https://wikimedia.org/api/rest_v1/media/math/render/svg/cc6e1f880981346a604257ebcacdef24c0aca2d6). Theoretically, the information from one token can propagate arbitrarily far down the sequence, if at every point the state continues to encode contextual information about the token. In practice this mechanism is flawed: the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem "Vanishing gradient problem") leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens. The dependency of token computations on the results of previous token computations also makes it hard to parallelize computation on modern deep-learning hardware. This can make the training of RNNs inefficient.

### Self-attention\[[edit](https://en.wikipedia.org/w/index.php?title=Transformer_(machine_learning_model)&action=edit&section=3 "Edit section: Self-attention")\]

These problems were addressed by attention mechanisms. Attention mechanisms let a model draw from the state at any preceding point along the sequence. The attention layer can access all previous states and weigh them according to a learned measure of relevance, providing relevant information about far-away tokens.

A clear example of the value of attention is in [language translation](https://en.wikipedia.org/wiki/Language_translation "Language translation"), where context is essential to assign the meaning of a word in a sentence. In an English-to-French translation system, the first word of the French output most probably depends heavily on the first few words of the English input. However, in a classic LSTM model, in order to produce the first word of the French output, the model is given only the state vector after processing the _last_ English word. Theoretically, this vector can encode information about the whole English sentence, giving the model all the necessary knowledge. In practice, this information is often poorly preserved by the LSTM. An attention mechanism can be added to address this problem: the decoder is given access to the state vectors of every English input word, not just the last, and can learn attention weights that dictate how much to attend to each English input state vector.

When added to RNNs, attention mechanisms increase performance. The development of the Transformer architecture revealed that attention mechanisms were powerful in themselves and that sequential recurrent processing of data was not necessary to achieve the quality gains of RNNs with attention. Transformers use an attention mechanism without an RNN, processing all tokens simultaneously and calculating attention weights between them in successive layers. Since the attention mechanism only uses information about other tokens from lower layers, it can be computed for all tokens in parallel, which leads to improved training speed.

## Architecture\[[edit](https://en.wikipedia.org/w/index.php?title=Transformer_(machine_learning_model)&action=edit&section=4 "Edit section: Architecture")\]

[![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/The-Transformer-model-architecture.png/400px-The-Transformer-model-architecture.png)](https://en.wikipedia.org/wiki/File:The-Transformer-model-architecture.png)

Transformer model architecture

### Input\[[edit](https://en.wikipedia.org/w/index.php?title=Transformer_(machine_learning_model)&action=edit&section=5 "Edit section: Input")\]

The input text is parsed into tokens by a [byte pair encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding "Byte pair encoding") [tokenizer](https://en.wikipedia.org/wiki/Lexical_analysis "Lexical analysis"), and each token is converted via a [word embedding](https://en.wikipedia.org/wiki/Word_embedding "Word embedding") into a vector. Then, positional information of the token is added to the word embedding.

### Encoder–decoder architecture\[[edit](https://en.wikipedia.org/w/index.php?title=Transformer_(machine_learning_model)&action=edit&section=6 "Edit section: Encoder–decoder architecture")\]

Like earlier [seq2seq](https://en.wikipedia.org/wiki/Seq2seq "Seq2seq") models, the original Transformer model used an **encoder–decoder** architecture. The encoder consists of encoding layers that process the input iteratively one layer after another, while the decoder consists of decoding layers that do the same thing to the encoder's output.

The function of each encoder layer is to generate encodings that contain information about which parts of the inputs are relevant to each other. It passes its encodings to the next encoder layer as inputs. Each decoder layer does the opposite, taking all the encodings and using their incorporated contextual information to generate an output sequence.<sup id="cite_ref-6"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-6">[6]</a></sup> To achieve this, each encoder and decoder layer makes use of an attention mechanism.

For each part of the input, attention weighs the relevance of every other part and draws from them to produce the output.<sup id="cite_ref-:1_7-0"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-:1-7">[7]</a></sup> Each decoder layer has an additional attention mechanism that draws information from the outputs of previous decoders, before the decoder layer draws information from the encodings.

Both the encoder and decoder layers have a [feed-forward neural network](https://en.wikipedia.org/wiki/Feedforward_neural_network "Feedforward neural network") for additional processing of the outputs and contain residual connections and layer normalization steps.<sup id="cite_ref-:1_7-1"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-:1-7">[7]</a></sup>

### Scaled dot-product attention\[[edit](https://en.wikipedia.org/w/index.php?title=Transformer_(machine_learning_model)&action=edit&section=7 "Edit section: Scaled dot-product attention")\]

The transformer building blocks are scaled dot-product [attention](https://en.wikipedia.org/wiki/Attention_(machine_learning) "Attention (machine learning)") units. When a sentence is passed into a transformer model, attention weights are calculated between every token simultaneously. The attention unit produces embeddings for every token in context that contain information about the token itself along with a weighted combination of other relevant tokens each weighted by its attention weight.

For each attention unit, the transformer model learns three weight matrices; the query weights ![{\displaystyle W_{Q}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/27233f4471dd458034969c094d1ac13bff1e38d1), the key weights ![{\displaystyle W_{K}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/1476608b04371ce36cdf6625d8b9ba8a96c615b9), and the value weights ![{\displaystyle W_{V}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/2ea6ed8ea3f0c61c33d4efb054e07f58b7046597). For each token ![i](https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20), the input [word embedding](https://en.wikipedia.org/wiki/Word_embedding "Word embedding") ![x_{i}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e87000dd6142b81d041896a30fe58f0c3acb2158) is multiplied with each of the three weight matrices to produce a query vector ![{\displaystyle q_{i}=x_{i}W_{Q}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/ebba814bf54791662c42b73973f13b95a062a835), a key vector ![{\displaystyle k_{i}=x_{i}W_{K}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c55d138a4e6a0203e8bac5d4f87d7143dec961ce), and a value vector ![{\displaystyle v_{i}=x_{i}W_{V}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/237f8999bf057b953208623a1647ade7ed908c52). Attention weights are calculated using the query and key vectors: the attention weight ![a_{ij}](https://wikimedia.org/api/rest_v1/media/math/render/svg/ebea6cd2813c330c798921a2894b358f7b643917) from token ![i](https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20) to token ![j](https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0) is the [dot product](https://en.wikipedia.org/wiki/Dot_product "Dot product") between ![q_{i}](https://wikimedia.org/api/rest_v1/media/math/render/svg/2752dcbff884354069fe332b8e51eb0a70a531b6) and ![k_j](https://wikimedia.org/api/rest_v1/media/math/render/svg/05ddf2c6d7759ac955e001a7cfafb2abfca41b0b). The attention weights are divided by the square root of the dimension of the key vectors, ![{\displaystyle {\sqrt {d_{k}}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0be678d1b945828faecd56b29927f5a60011be37), which stabilizes gradients during training, and passed through a [softmax](https://en.wikipedia.org/wiki/Softmax_function "Softmax function") which normalizes the weights. The fact that ![{\displaystyle W_{Q}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/27233f4471dd458034969c094d1ac13bff1e38d1) and ![{\displaystyle W_{K}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/1476608b04371ce36cdf6625d8b9ba8a96c615b9) are different matrices allows attention to be non-symmetric: if token ![i](https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20) attends to token ![j](https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0) (i.e. ![{\displaystyle q_{i}\cdot k_{j}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7de4219a59ace005d92f8d0a13466dbdb5fd6d9c) is large), this does not necessarily mean that token ![j](https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0) will attend to token ![i](https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20) (i.e. ![{\displaystyle q_{j}\cdot k_{i}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/d40445a57203e20510d7b629e0567957524700e4) could be small). The output of the attention unit for token ![i](https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20) is the weighted sum of the value vectors of all tokens, weighted by ![a_{ij}](https://wikimedia.org/api/rest_v1/media/math/render/svg/ebea6cd2813c330c798921a2894b358f7b643917), the attention from token ![i](https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20) to each token.

The attention calculation for all tokens can be expressed as one large matrix calculation using the [softmax function](https://en.wikipedia.org/wiki/Softmax_function "Softmax function"), which is useful for training due to computational matrix operation optimizations that quickly compute matrix operations. The matrices ![Q](https://wikimedia.org/api/rest_v1/media/math/render/svg/8752c7023b4b3286800fe3238271bbca681219ed), ![K](https://wikimedia.org/api/rest_v1/media/math/render/svg/2b76fce82a62ed5461908f0dc8f037de4e3686b0) and ![V](https://wikimedia.org/api/rest_v1/media/math/render/svg/af0f6064540e84211d0ffe4dac72098adfa52845) are defined as the matrices where the ![i](https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20)th rows are vectors ![q_{i}](https://wikimedia.org/api/rest_v1/media/math/render/svg/2752dcbff884354069fe332b8e51eb0a70a531b6), ![k_{i}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f29138ed3ad54ffce527daccadc49c520459b0b0), and ![v_{i}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7dffe5726650f6daac54829972a94f38eb8ec127) respectively.

![{\displaystyle {\begin{aligned}{\text{Attention}}(Q,K,V)={\text{softmax}}\left({\frac {QK^{\mathrm {T} }}{\sqrt {d_{k}}}}\right)V\end{aligned}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0b2afc7240eb97375a384b1628c18438e3068e3f)

where softmax is taken over the horizontal axis.

#### Multi-head attention\[[edit](https://en.wikipedia.org/w/index.php?title=Transformer_(machine_learning_model)&action=edit&section=8 "Edit section: Multi-head attention")\]

One set of ![{\displaystyle \left(W_{Q},W_{K},W_{V}\right)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/021c670a99024a281521fcfdc56d59571a70e4fd) matrices is called an _attention head_, and each layer in a transformer model has multiple attention heads. While each attention head attends to the tokens that are relevant to each token, with multiple attention heads the model can do this for different definitions of "relevance". In addition, the influence field representing relevance can become progressively dilated in successive layers. Many transformer attention heads encode relevance relations that are meaningful to humans. For example, some attention heads can attend mostly to the next word, while others mainly attend from verbs to their direct objects.<sup id="cite_ref-8"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-8">[8]</a></sup> The computations for each attention head can be performed in [parallel](https://en.wikipedia.org/wiki/Parallel_computing "Parallel computing"), which allows for fast processing. The outputs for the attention layer are concatenated to pass into the [feed-forward neural network](https://en.wikipedia.org/wiki/Feedforward_neural_network "Feedforward neural network") layers.

Concretely, let the multiple attention heads be indexed by ![i](https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20), then we have

![{\displaystyle {\text{MultiheadedAttention}}(Q,K,V)={\text{Concat}}({\text{Attention}}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V}))W^{O}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/bff7290d900767d8763630f0460266064c636f3e)

where the matrices ![{\displaystyle W_{i}^{Q},W_{i}^{K},W_{i}^{V}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3087f1739ddc134719d701163d3a75af985498b1) are "projection matrices" owned by individual attention head ![i](https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20), and ![{\displaystyle W^{O}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7f0f376fd1863b3a195a6bf34b56ff43ec5d695f) is a final projection matrix owned by the whole multi-headed attention head.

#### Masked attention\[[edit](https://en.wikipedia.org/w/index.php?title=Transformer_(machine_learning_model)&action=edit&section=9 "Edit section: Masked attention")\]

It may be necessary to cut out attention links between some word-pairs. For example, the decoder for token position ![t](https://wikimedia.org/api/rest_v1/media/math/render/svg/65658b7b223af9e1acc877d848888ecdb4466560) should not have access to token position ![t+1](https://wikimedia.org/api/rest_v1/media/math/render/svg/ab2785d8415d6902b0c93efe1419c4bc3ce4643d). This may be accomplished before the softmax stage by adding a mask matrix ![M](https://wikimedia.org/api/rest_v1/media/math/render/svg/f82cade9898ced02fdd08712e5f0c0151758a0dd) that is negative infinity at entries where the attention link must be cut, and zero at other places.

### Encoder\[[edit](https://en.wikipedia.org/w/index.php?title=Transformer_(machine_learning_model)&action=edit&section=10 "Edit section: Encoder")\]

Each encoder consists of two major components: a self-attention mechanism and a feed-forward neural network. The self-attention mechanism accepts input encodings from the previous encoder and weights their relevance to each other to generate output encodings. The feed-forward neural network further processes each output encoding individually. These output encodings are then passed to the next encoder as its input, as well as to the decoders.

The first encoder takes positional information and [embeddings](https://en.wikipedia.org/wiki/Word_embedding "Word embedding") of the input sequence as its input, rather than encodings. The positional information is necessary for the transformer to make use of the order of the sequence, because no other part of the transformer makes use of this.<sup id="cite_ref-:0_1-4"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-:0-1">[1]</a></sup>

The encoder is bidirectional. Attention can be placed on tokens before and after the current token. Tokens are used instead of words to account for [polysemy](https://en.wikipedia.org/wiki/Word_embedding#Polysemy_and_homonymy "Word embedding").

[![](https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/Positional_encoding.png/220px-Positional_encoding.png)](https://en.wikipedia.org/wiki/File:Positional_encoding.png)

A diagram of a [sinusoidal](https://en.wikipedia.org/wiki/Sine_wave "Sine wave") positional encoding with parameters ![{\displaystyle N=10000,d=100}](https://wikimedia.org/api/rest_v1/media/math/render/svg/fe83017b9728026bf6d2bae4a357041d198ac494)

#### Positional encoding\[[edit](https://en.wikipedia.org/w/index.php?title=Transformer_(machine_learning_model)&action=edit&section=11 "Edit section: Positional encoding")\]

A positional encoding is a fixed-size vector representation that encapsulates the relative positions of tokens within a target sequence: it provides the transformer model with information about _where_ the words are in the input sequence.

The positional encoding is defined as a function of type ![{\displaystyle f:\mathbb {R} \to \mathbb {R} ^{d};d\in \mathbb {Z} ,d>0}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f97ea00a952cdbf0e678b544c0986c588571d9d4), where ![d](https://wikimedia.org/api/rest_v1/media/math/render/svg/e85ff03cbe0c7341af6b982e47e9f90d235c66ab) is a positive even [integer](https://en.wikipedia.org/wiki/Integer "Integer"). The full positional encoding - as defined in the original paper - is given by the equation:

![{\displaystyle (f(t)_{2k},f(t)_{2k+1})=(\sin(\theta ),\cos(\theta ))\quad \forall k\in \{0,1,\ldots ,d/2-1\}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/cbca45061217292a4920ea20881b77a1b39a41ea)

where ![{\displaystyle \theta ={\frac {t}{r^{k}}},r=N^{2/d}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/8ba16328b4d94e923d80ad257002a6d18deb2edb).

  
Here, ![N](https://wikimedia.org/api/rest_v1/media/math/render/svg/f5e3890c981ae85503089652feb48b191b57aae3) is a free parameter that should be significantly larger than the biggest ![k](https://wikimedia.org/api/rest_v1/media/math/render/svg/c3c9a2c7b599b37105512c5d570edc034056dd40) that would be input into the positional encoding function. In the original paper,<sup id="cite_ref-:0_1-5"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-:0-1">[1]</a></sup> the authors chose ![{\displaystyle N=10000}](https://wikimedia.org/api/rest_v1/media/math/render/svg/8f13b17e1a7fbe046ab619ccc5167809d04872c2).

The function is in a simpler form when written as a complex function of type ![{\displaystyle f:\mathbb {R} \to \mathbb {C} ^{d/2}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/52c816b7a3f4f0855fc1cc7bafb9dbce1efc09d0)

![{\displaystyle f(t)=\left(e^{it/r^{k}}\right)_{k=0,1,\ldots ,{\frac {d}{2}}-1}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4887fec07bd9ef29ad5783f32651b1e503a88130)

where ![{\displaystyle r=N^{2/d}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/11cb2cfb2ab25e2e0cb3ed51071f1e7f38060c99).

The main reason the authors chose this as the positional encoding function is that it allows one to perform shifts as linear transformations:

![{\displaystyle f(t+\Delta t)=\mathrm {diag} (f(\Delta t))f(t)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/cce4053e6d0f3e225b153bc362be4970c3e9b535)

where ![{\displaystyle \Delta t\in \mathbb {R} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/40be374d0e9f96fefe0a14ddb46218a42409b2a6) is the distance one wishes to shift. This allows the transformer to take any encoded position, and find the encoding of the position n-steps-ahead or n-steps-behind, by a matrix multiplication.

By taking a linear sum, any convolution can also be implemented as linear transformations:

![{\displaystyle \sum _{j}c_{j}f(t+\Delta t_{j})=\left(\sum _{j}c_{j}\,\mathrm {diag} (f(\Delta t_{j}))\right)f(t)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/14a5ba5df2e142a7c6812dd345b2001550cfa3f0)

for any constants ![c_{j}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a844d180d176af828d1636d4e85aa534d0b77baa). This allows the transformer to take any encoded position and find a linear sum of the encoded locations of its neighbors. This sum of encoded positions, when fed into the attention mechanism, would create attention weights on its neighbors, much like what happens in a [convolutional neural network](https://en.wikipedia.org/wiki/Convolutional_neural_network "Convolutional neural network") [language model](https://en.wikipedia.org/wiki/Language_model "Language model"). In the author's words, "we hypothesized it would allow the model to easily learn to attend by relative position".

In typical implementations, all operations are done over the real numbers, not the complex numbers, but since [complex multiplication can be implemented as real 2-by-2 matrix multiplication](https://en.wikipedia.org/wiki/Complex_number#Matrix_representation_of_complex_numbers "Complex number"), this is a mere notational difference.

Other positional encoding schemes exist.<sup id="cite_ref-9"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-9">[9]</a></sup>

### Decoder\[[edit](https://en.wikipedia.org/w/index.php?title=Transformer_(machine_learning_model)&action=edit&section=12 "Edit section: Decoder")\]

Each decoder consists of three major components: a self-attention mechanism, an attention mechanism over the encodings, and a feed-forward neural network. The decoder functions in a similar fashion to the encoder, but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders. This mechanism can also be called the _encoder-decoder attention_.<sup id="cite_ref-:0_1-6"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-:0-1">[1]</a></sup><sup id="cite_ref-:1_7-2"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-:1-7">[7]</a></sup>

Like the first encoder, the first decoder takes positional information and embeddings of the output sequence as its input, rather than encodings. The transformer must not use the current or future output to predict an output, so the output sequence must be partially masked to prevent this reverse information flow.<sup id="cite_ref-:0_1-7"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-:0-1">[1]</a></sup> This allows for [autoregressive](https://en.wikipedia.org/wiki/Autoregressive_model "Autoregressive model") text generation. For all attention heads, attention can't be placed on following tokens. The last decoder is followed by a final [linear transformation](https://en.wikipedia.org/wiki/Matrix_multiplication "Matrix multiplication") and [softmax layer](https://en.wikipedia.org/wiki/Softmax_function "Softmax function"), to produce the output probabilities over the vocabulary.

[GPT](https://en.wikipedia.org/wiki/GPT-3 "GPT-3") has a decoder-only architecture.

### Alternatives\[[edit](https://en.wikipedia.org/w/index.php?title=Transformer_(machine_learning_model)&action=edit&section=13 "Edit section: Alternatives")\]

Training transformer-based architectures can be expensive, especially for long inputs.<sup id="cite_ref-reformer_10-0"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-reformer-10">[10]</a></sup> Alternative architectures include the Reformer (which reduces the computational load from ![O(N^{2})](https://wikimedia.org/api/rest_v1/media/math/render/svg/e5d43a3df904fa4d7220f5b86285298aa36d969b) to ![O(N\ln N)](https://wikimedia.org/api/rest_v1/media/math/render/svg/d3d19d1f2923ba0d7170ade3df165c0de1d2423e)<sup id="cite_ref-reformer_10-1"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-reformer-10">[10]</a></sup>), or models like ETC/BigBird (which can reduce it to ![O(N)](https://wikimedia.org/api/rest_v1/media/math/render/svg/78484c5c26cfc97bb3b915418caa09454421e80b))<sup id="cite_ref-11"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-11">[11]</a></sup> where ![N](https://wikimedia.org/api/rest_v1/media/math/render/svg/f5e3890c981ae85503089652feb48b191b57aae3) is the length of the sequence. This is done using [locality-sensitive hashing](https://en.wikipedia.org/wiki/Locality-sensitive_hashing "Locality-sensitive hashing") and reversible layers.<sup id="cite_ref-12"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-12">[12]</a></sup><sup id="cite_ref-13"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-13">[13]</a></sup>

Ordinary transformers require a memory size that is quadratic in the size of the context window. Attention Free Transformers<sup id="cite_ref-14"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-14">[14]</a></sup> reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value.

A benchmark for comparing transformer architectures was introduced in late 2020 by the name of _Long Range Arena_.<sup id="cite_ref-15"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-15">[15]</a></sup>

## Training\[[edit](https://en.wikipedia.org/w/index.php?title=Transformer_(machine_learning_model)&action=edit&section=14 "Edit section: Training")\]

### Methods for stabilizing training\[[edit](https://en.wikipedia.org/w/index.php?title=Transformer_(machine_learning_model)&action=edit&section=15 "Edit section: Methods for stabilizing training")\]

The plain Transformer architecture has difficulty converging. In the original paper<sup id="cite_ref-:0_1-8"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-:0-1">[1]</a></sup> the authors recommended using learning rate warmup. That is, the learning rate should linearly scale up from 0 to maximal value for the first part of the training (usually recommended to be 2% of the total number of training steps), before decaying again.

<sup id="cite_ref-16"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-16">[16]</a></sup> found that using layer normalization _before_ (instead of after) multiheaded attention and feedforward layers stabilizes training, not requiring learning rate warmup.

### Pretrain-finetune\[[edit](https://en.wikipedia.org/w/index.php?title=Transformer_(machine_learning_model)&action=edit&section=16 "Edit section: Pretrain-finetune")\]

Transformers typically undergo [self-supervised learning](https://en.wikipedia.org/wiki/Self-supervised_learning "Self-supervised learning") involving [unsupervised](https://en.wikipedia.org/wiki/Unsupervised_learning "Unsupervised learning") pretraining followed by [supervised](https://en.wikipedia.org/wiki/Supervised_learning "Supervised learning") fine-tuning. Pretraining is typically done on a larger dataset than fine-tuning, due to the limited availability of labeled training data. Tasks for pretraining and fine-tuning commonly include:

-   [language modeling](https://en.wikipedia.org/wiki/Language_modeling "Language modeling")<sup id="cite_ref-:6_4-1"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-:6-4">[4]</a></sup>
-   next-sentence prediction<sup id="cite_ref-:6_4-2"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-:6-4">[4]</a></sup>
-   [question answering](https://en.wikipedia.org/wiki/Question_answering "Question answering")<sup id="cite_ref-:7_5-1"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-:7-5">[5]</a></sup>
-   [reading comprehension](https://en.wikipedia.org/wiki/Natural-language_understanding "Natural-language understanding")
-   [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis "Sentiment analysis")<sup id="cite_ref-:8_17-0"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-:8-17">[17]</a></sup>
-   [paraphrasing](https://en.wikipedia.org/wiki/Text_Summaries "Text Summaries")<sup id="cite_ref-:8_17-1"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-:8-17">[17]</a></sup>

## Applications\[[edit](https://en.wikipedia.org/w/index.php?title=Transformer_(machine_learning_model)&action=edit&section=17 "Edit section: Applications")\]

The transformer has had great success in [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing "Natural language processing") (NLP), for example the tasks of [machine translation](https://en.wikipedia.org/wiki/Machine_translation "Machine translation") and [time series](https://en.wikipedia.org/wiki/Time_series "Time series") prediction. Many pretrained models such as [GPT-2](https://en.wikipedia.org/wiki/GPT-2 "GPT-2"), [GPT-3](https://en.wikipedia.org/wiki/GPT-3 "GPT-3"), [BERT](https://en.wikipedia.org/wiki/BERT_(language_model) "BERT (language model)"), XLNet, RoBERTa and [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT "ChatGPT") demonstrate the ability of transformers to perform a wide variety of such NLP-related tasks, and have the potential to find real-world applications. These may include:

-   [machine translation](https://en.wikipedia.org/wiki/Machine_translation "Machine translation")
-   [document summarization](https://en.wikipedia.org/wiki/Automatic_summarization "Automatic summarization")
-   [document generation](https://en.wikipedia.org/wiki/Natural_language_generation "Natural language generation")
-   [named entity recognition](https://en.wikipedia.org/wiki/Named-entity_recognition "Named-entity recognition") (NER)
-   [biological sequence analysis](https://en.wikipedia.org/wiki/Sequence_analysis "Sequence analysis")
-   [video understanding](https://en.wikipedia.org/wiki/Computer_vision "Computer vision").

## Implementations\[[edit](https://en.wikipedia.org/w/index.php?title=Transformer_(machine_learning_model)&action=edit&section=18 "Edit section: Implementations")\]

The transformer model has been implemented in standard deep learning [frameworks](https://en.wikipedia.org/wiki/Framework_(computer_science) "Framework (computer science)") such as [TensorFlow](https://en.wikipedia.org/wiki/TensorFlow "TensorFlow") and [PyTorch](https://en.wikipedia.org/wiki/PyTorch "PyTorch").

_Transformers_ is a library produced by [Hugging Face](https://en.wikipedia.org/wiki/Hugging_Face "Hugging Face") that supplies transformer-based architectures and pretrained models.<sup id="cite_ref-wolf2020_3-1"><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_note-wolf2020-3">[3]</a></sup>

## See also\[[edit](https://en.wikipedia.org/w/index.php?title=Transformer_(machine_learning_model)&action=edit&section=19 "Edit section: See also")\]

-   [Perceiver](https://en.wikipedia.org/wiki/Perceiver "Perceiver") – Machine learning algorithm for non-textual data
-   [BERT (language model)](https://en.wikipedia.org/wiki/BERT_(language_model) "BERT (language model)") – A masked neural language model developed by Google.
-   [GPT-3](https://en.wikipedia.org/wiki/GPT-3 "GPT-3") – 2020 text-generating language model
-   [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT "ChatGPT") – Artificial intelligence chatbot developed by OpenAI
-   [Wu Dao](https://en.wikipedia.org/wiki/Wu_Dao "Wu Dao") – Chinese multimodal artificial intelligence program
-   [Vision transformer](https://en.wikipedia.org/wiki/Vision_transformer "Vision transformer") – Machine learning algorithm for vision processing
-   [BLOOM (language model)](https://en.wikipedia.org/wiki/BLOOM_(language_model) "BLOOM (language model)") – Open-access multilingual language model

## References\[[edit](https://en.wikipedia.org/w/index.php?title=Transformer_(machine_learning_model)&action=edit&section=20 "Edit section: References")\]

1.  ^ [Jump up to: <sup><i><b>a</b></i></sup>](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-:0_1-0) [<sup><i><b>b</b></i></sup>](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-:0_1-1) [<sup><i><b>c</b></i></sup>](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-:0_1-2) [<sup><i><b>d</b></i></sup>](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-:0_1-3) [<sup><i><b>e</b></i></sup>](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-:0_1-4) [<sup><i><b>f</b></i></sup>](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-:0_1-5) [<sup><i><b>g</b></i></sup>](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-:0_1-6) [<sup><i><b>h</b></i></sup>](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-:0_1-7) [<sup><i><b>i</b></i></sup>](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-:0_1-8) Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N.; Kaiser, Lukasz; Polosukhin, Illia (2017-06-12). "Attention Is All You Need". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) "ArXiv (identifier)"):[1706.03762](https://arxiv.org/abs/1706.03762) \[[cs.CL](https://arxiv.org/archive/cs.CL)\].
2.  **[^](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-2 "Jump up")** He, Cheng (31 December 2021). ["Transformer in CV"](https://towardsdatascience.com/transformer-in-cv-bbdb58bf335e). _Transformer in CV_. Towards Data Science.
3.  ^ [Jump up to: <sup><i><b>a</b></i></sup>](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-wolf2020_3-0) [<sup><i><b>b</b></i></sup>](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-wolf2020_3-1) Wolf, Thomas; Debut, Lysandre; Sanh, Victor; Chaumond, Julien; Delangue, Clement; Moi, Anthony; Cistac, Pierric; Rault, Tim; Louf, Remi; Funtowicz, Morgan; Davison, Joe; Shleifer, Sam; von Platen, Patrick; Ma, Clara; Jernite, Yacine; Plu, Julien; Xu, Canwen; Le Scao, Teven; Gugger, Sylvain; Drame, Mariama; Lhoest, Quentin; Rush, Alexander (2020). "Transformers: State-of-the-Art Natural Language Processing". _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_. pp. 38–45. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.18653/v1/2020.emnlp-demos.6](https://doi.org/10.18653%2Fv1%2F2020.emnlp-demos.6). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)") [208117506](https://api.semanticscholar.org/CorpusID:208117506).
4.  ^ [Jump up to: <sup><i><b>a</b></i></sup>](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-:6_4-0) [<sup><i><b>b</b></i></sup>](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-:6_4-1) [<sup><i><b>c</b></i></sup>](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-:6_4-2) ["Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing"](http://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html). _Google AI Blog_. Retrieved 2019-08-25.
5.  ^ [Jump up to: <sup><i><b>a</b></i></sup>](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-:7_5-0) [<sup><i><b>b</b></i></sup>](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-:7_5-1) ["Better Language Models and Their Implications"](https://openai.com/blog/better-language-models/). _OpenAI_. 2019-02-14. Retrieved 2019-08-25.
6.  **[^](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-6 "Jump up")** ["Sequence Modeling with Neural Networks (Part 2): Attention Models"](https://indico.io/blog/sequence-modeling-neural-networks-part2-attention-models/). _Indico_. 2016-04-18. Retrieved 2019-10-15.
7.  ^ [Jump up to: <sup><i><b>a</b></i></sup>](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-:1_7-0) [<sup><i><b>b</b></i></sup>](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-:1_7-1) [<sup><i><b>c</b></i></sup>](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-:1_7-2) Alammar, Jay. ["The Illustrated Transformer"](http://jalammar.github.io/illustrated-transformer/). _jalammar.github.io_. Retrieved 2019-10-15.
8.  **[^](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-8 "Jump up")** Clark, Kevin; Khandelwal, Urvashi; Levy, Omer; Manning, Christopher D. (August 2019). ["What Does BERT Look at? An Analysis of BERT's Attention"](https://www.aclweb.org/anthology/W19-4828). _Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP_. Florence, Italy: Association for Computational Linguistics: 276–286. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.18653/v1/W19-4828](https://doi.org/10.18653%2Fv1%2FW19-4828).
9.  **[^](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-9 "Jump up")** Dufter, Philipp; Schmitt, Martin; Schütze, Hinrich (2022-06-06). ["Position Information in Transformers: An Overview"](https://doi.org/10.1162%2Fcoli_a_00445). _Computational Linguistics_. **48** (3): 733–763. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1162/coli\_a\_00445](https://doi.org/10.1162%2Fcoli_a_00445). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) "ISSN (identifier)") [0891-2017](https://www.worldcat.org/issn/0891-2017). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)") [231986066](https://api.semanticscholar.org/CorpusID:231986066).
10.  ^ [Jump up to: <sup><i><b>a</b></i></sup>](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-reformer_10-0) [<sup><i><b>b</b></i></sup>](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-reformer_10-1) Kitaev, Nikita; Kaiser, Łukasz; Levskaya, Anselm (2020). "Reformer: The Efficient Transformer". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) "ArXiv (identifier)"):[2001.04451](https://arxiv.org/abs/2001.04451) \[[cs.LG](https://arxiv.org/archive/cs.LG)\].
11.  **[^](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-11 "Jump up")** ["Constructing Transformers For Longer Sequences with Sparse Attention Methods"](https://ai.googleblog.com/2021/03/constructing-transformers-for-longer.html). _Google AI Blog_. Retrieved 2021-05-28.
12.  **[^](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-12 "Jump up")** ["Tasks with Long Sequences – Chatbot"](https://www.coursera.org/lecture/attention-models-in-nlp/tasks-with-long-sequences-suzNH). _Coursera_.
13.  **[^](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-13 "Jump up")** ["Reformer: The Efficient Transformer"](http://ai.googleblog.com/2020/01/reformer-efficient-transformer.html). _Google AI Blog_. Retrieved 2020-10-22.
14.  **[^](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-14 "Jump up")** Zhai, Shuangfei; Talbott, Walter; Srivastava, Nitish; Huang, Chen; Goh, Hanlin; Zhang, Ruixiang; Susskind, Josh (2021-09-21). "An Attention Free Transformer". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) "ArXiv (identifier)"):[2105.14103](https://arxiv.org/abs/2105.14103) \[[cs.LG](https://arxiv.org/archive/cs.LG)\].
15.  **[^](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-15 "Jump up")** Tay, Yi; Dehghani, Mostafa; Abnar, Samira; Shen, Yikang; Bahri, Dara; Pham, Philip; Rao, Jinfeng; Yang, Liu; Ruder, Sebastian; Metzler, Donald (2020-11-08). "Long Range Arena: A Benchmark for Efficient Transformers". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) "ArXiv (identifier)"):[2011.04006](https://arxiv.org/abs/2011.04006) \[[cs.LG](https://arxiv.org/archive/cs.LG)\].
16.  **[^](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-16 "Jump up")** Xiong, Ruibin; Yang, Yunchang; He, Di; Zheng, Kai; Zheng, Shuxin; Xing, Chen; Zhang, Huishuai; Lan, Yanyan; Wang, Liwei; Liu, Tie-Yan (2020-06-29). ["On Layer Normalization in the Transformer Architecture"](http://arxiv.org/abs/2002.04745). _arXiv:2002.04745 \[cs, stat\]_.
17.  ^ [Jump up to: <sup><i><b>a</b></i></sup>](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-:8_17-0) [<sup><i><b>b</b></i></sup>](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#cite_ref-:8_17-1) Wang, Alex; Singh, Amanpreet; Michael, Julian; Hill, Felix; Levy, Omer; Bowman, Samuel (2018). "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding". _Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP_. Stroudsburg, PA, USA: Association for Computational Linguistics: 353–355. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) "ArXiv (identifier)"):[1804.07461](https://arxiv.org/abs/1804.07461). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.18653/v1/w18-5446](https://doi.org/10.18653%2Fv1%2Fw18-5446). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)") [5034059](https://api.semanticscholar.org/CorpusID:5034059).

## Further reading\[[edit](https://en.wikipedia.org/w/index.php?title=Transformer_(machine_learning_model)&action=edit&section=21 "Edit section: Further reading")\]


# related stuff

[[machine translation]]