


# wiki-Instruction set architecture


---
created: 2023-04-13T16:06:06 (UTC +08:00)
tags: []
source: https://en.wikipedia.org/wiki/Instruction_set_architecture
author: Contributors to Wikimedia projects
---

# Instruction set architecture - Wikipedia

> ## Excerpt
> In computer science, an instruction set architecture (ISA), also called computer architecture, is an abstract model of a computer. A device that executes instructions described by that ISA, such as a central processing unit (CPU), is called an implementation.

---
In [computer science](https://en.wikipedia.org/wiki/Computer_science "Computer science"), an **instruction set architecture** (**ISA**), also called **[computer architecture](https://en.wikipedia.org/wiki/Computer_architecture "Computer architecture"),** is an [abstract model](https://en.wikipedia.org/wiki/Abstract_model "Abstract model") of a [computer](https://en.wikipedia.org/wiki/Computer "Computer"). A device that executes instructions described by that ISA, such as a [central processing unit](https://en.wikipedia.org/wiki/Central_processing_unit "Central processing unit") (CPU), is called an _implementation_.

In general, an ISA defines the supported [instructions](https://en.wikipedia.org/wiki/Machine_code "Machine code"), [data types](https://en.wikipedia.org/wiki/Data_type "Data type"), [registers](https://en.wikipedia.org/wiki/Register_(computer) "Register (computer)"), the hardware support for managing [main memory](https://en.wikipedia.org/wiki/Random-access_memory "Random-access memory"), fundamental features (such as the [memory consistency](https://en.wikipedia.org/wiki/Memory_consistency "Memory consistency"), [addressing modes](https://en.wikipedia.org/wiki/Addressing_mode "Addressing mode"), [virtual memory](https://en.wikipedia.org/wiki/Virtual_memory "Virtual memory")), and the [input/output](https://en.wikipedia.org/wiki/Input/output "Input/output") model of a family of implementations of the ISA.

An ISA specifies the behavior of [machine code](https://en.wikipedia.org/wiki/Machine_code "Machine code") running on implementations of that ISA in a fashion that does not depend on the characteristics of that implementation, providing [binary compatibility](https://en.wikipedia.org/wiki/Binary_compatibility "Binary compatibility") between implementations. This enables multiple implementations of an ISA that differ in characteristics such as [performance](https://en.wikipedia.org/wiki/Computer_performance "Computer performance"), physical size, and monetary cost (among other things), but that are capable of running the same machine code, so that a lower-performance, lower-cost machine can be replaced with a higher-cost, higher-performance machine without having to replace software. It also enables the evolution of the [microarchitectures](https://en.wikipedia.org/wiki/Microarchitecture "Microarchitecture") of the implementations of that ISA, so that a newer, higher-performance implementation of an ISA can run software that runs on previous generations of implementations.

If an [operating system](https://en.wikipedia.org/wiki/Operating_system "Operating system") maintains a standard and compatible [application binary interface](https://en.wikipedia.org/wiki/Application_binary_interface "Application binary interface") (ABI) for a particular ISA, machine code will run on future implementations of that ISA and operating system. However, if an ISA supports running multiple operating systems, it does not guarantee that machine code for one operating system will run on another operating system, unless the first operating system supports running machine code built for the other operating system.

An ISA can be extended by adding instructions or other capabilities, or adding support for larger addresses and data values; an implementation of the extended ISA will still be able to execute machine code for versions of the ISA without those extensions. Machine code using those extensions will only run on implementations that support those extensions.

The binary compatibility that they provide makes ISAs one of the most fundamental abstractions in [computing](https://en.wikipedia.org/wiki/Computing "Computing").

## Overview\[[edit](https://en.wikipedia.org/w/index.php?title=Instruction_set_architecture&action=edit&section=1 "Edit section: Overview")\]

An instruction set architecture is distinguished from a [microarchitecture](https://en.wikipedia.org/wiki/Microarchitecture "Microarchitecture"), which is the set of [processor design](https://en.wikipedia.org/wiki/Processor_design "Processor design") techniques used, in a particular processor, to implement the instruction set. Processors with different microarchitectures can share a common instruction set. For example, the [Intel](https://en.wikipedia.org/wiki/Intel "Intel") [Pentium](https://en.wikipedia.org/wiki/P5_(microarchitecture) "P5 (microarchitecture)") and the [AMD](https://en.wikipedia.org/wiki/Advanced_Micro_Devices "Advanced Micro Devices") [Athlon](https://en.wikipedia.org/wiki/Athlon "Athlon") implement nearly identical versions of the [x86 instruction set](https://en.wikipedia.org/wiki/X86_instruction_set "X86 instruction set"), but they have radically different internal designs.

The concept of an _architecture_, distinct from the design of a specific machine, was developed by [Fred Brooks](https://en.wikipedia.org/wiki/Fred_Brooks "Fred Brooks") at IBM during the design phase of [System/360](https://en.wikipedia.org/wiki/System/360 "System/360").

> Prior to NPL \[System/360\], the company's computer designers had been free to honor cost objectives not only by selecting technologies but also by fashioning functional and architectural refinements. The SPREAD compatibility objective, in contrast, postulated a single architecture for a series of five processors spanning a wide range of cost and performance. None of the five engineering design teams could count on being able to bring about adjustments in architectural specifications as a way of easing difficulties in achieving cost and performance objectives.<sup id="cite_ref-Pugh_1-0"><a href="https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_note-Pugh-1">[1]</a></sup><sup><span title="Page / location: p.137">: p.137 </span></sup> 

Some [virtual machines](https://en.wikipedia.org/wiki/Virtual_machine "Virtual machine") that support [bytecode](https://en.wikipedia.org/wiki/Bytecode "Bytecode") as their ISA such as [Smalltalk](https://en.wikipedia.org/wiki/Smalltalk "Smalltalk"), the [Java virtual machine](https://en.wikipedia.org/wiki/Java_virtual_machine "Java virtual machine"), and [Microsoft](https://en.wikipedia.org/wiki/Microsoft "Microsoft")'s [Common Language Runtime](https://en.wikipedia.org/wiki/Common_Language_Runtime "Common Language Runtime"), implement this by translating the bytecode for commonly used code paths into native machine code. In addition, these virtual machines execute less frequently used code paths by interpretation (see: [Just-in-time compilation](https://en.wikipedia.org/wiki/Just-in-time_compilation "Just-in-time compilation")). [Transmeta](https://en.wikipedia.org/wiki/Transmeta "Transmeta") implemented the x86 instruction set atop [VLIW](https://en.wikipedia.org/wiki/VLIW "VLIW") processors in this fashion.

## Classification of ISAs\[[edit](https://en.wikipedia.org/w/index.php?title=Instruction_set_architecture&action=edit&section=2 "Edit section: Classification of ISAs")\]

An ISA may be classified in a number of different ways. A common classification is by architectural _complexity_. A [complex instruction set computer](https://en.wikipedia.org/wiki/Complex_instruction_set_computer "Complex instruction set computer") (CISC) has many specialized instructions, some of which may only be rarely used in practical programs. A [reduced instruction set computer](https://en.wikipedia.org/wiki/Reduced_instruction_set_computer "Reduced instruction set computer") (RISC) simplifies the processor by efficiently implementing only the instructions that are frequently used in programs, while the less common operations are implemented as subroutines, having their resulting additional processor execution time offset by infrequent use.<sup id="cite_ref-2"><a href="https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_note-2">[2]</a></sup>

Other types include [very long instruction word](https://en.wikipedia.org/wiki/Very_long_instruction_word "Very long instruction word") (VLIW) architectures, and the closely related _long instruction word_ (LIW) and<sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (March 2023)">citation needed</span></a></i>]</sup> _[explicitly parallel instruction computing](https://en.wikipedia.org/wiki/Explicitly_parallel_instruction_computing "Explicitly parallel instruction computing")_ (EPIC) architectures. These architectures seek to exploit [instruction-level parallelism](https://en.wikipedia.org/wiki/Instruction-level_parallelism "Instruction-level parallelism") with less hardware than RISC and CISC by making the [compiler](https://en.wikipedia.org/wiki/Compiler "Compiler") responsible for instruction issue and scheduling.<sup id="cite_ref-3"><a href="https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_note-3">[3]</a></sup>

Architectures with even less complexity have been studied, such as the [minimal instruction set computer](https://en.wikipedia.org/wiki/Minimal_instruction_set_computer "Minimal instruction set computer") (MISC) and [one-instruction set computer](https://en.wikipedia.org/wiki/One-instruction_set_computer "One-instruction set computer") (OISC). These are theoretically important types, but have not been commercialized.<sup id="cite_ref-4"><a href="https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_note-4">[4]</a></sup><sup id="cite_ref-5"><a href="https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_note-5">[5]</a></sup>

## Instructions\[[edit](https://en.wikipedia.org/w/index.php?title=Instruction_set_architecture&action=edit&section=3 "Edit section: Instructions")\]

[Machine language](https://en.wikipedia.org/wiki/Machine_code "Machine code") is built up from discrete _statements_ or _instructions_. On the processing architecture, a given instruction may specify:

-   [opcode](https://en.wikipedia.org/wiki/Opcode "Opcode") (the instruction to be performed) e.g. add, copy, test
-   any explicit operands:

[registers](https://en.wikipedia.org/wiki/Processor_register "Processor register")

literal/constant values

[addressing modes](https://en.wikipedia.org/wiki/Addressing_mode "Addressing mode") used to access memory

More complex operations are built up by combining these simple instructions, which are executed sequentially, or as otherwise directed by [control flow](https://en.wikipedia.org/wiki/Control_flow "Control flow") instructions.

### Instruction types\[[edit](https://en.wikipedia.org/w/index.php?title=Instruction_set_architecture&action=edit&section=4 "Edit section: Instruction types")\]

Examples of operations common to many instruction sets include:

#### Data handling and memory operations\[[edit](https://en.wikipedia.org/w/index.php?title=Instruction_set_architecture&action=edit&section=5 "Edit section: Data handling and memory operations")\]

-   _Set_ a [register](https://en.wikipedia.org/wiki/Processor_register "Processor register") to a fixed constant value.
-   _Copy_ data from a memory location or a register to a memory location or a register (a machine instruction is often called _move_; however, the term is misleading). They are used to store the contents of a register, the contents of another memory location or the result of a computation, or to retrieve stored data to perform a computation on it later. They are often called [load and store](https://en.wikipedia.org/wiki/Load_and_store "Load and store") operations.
-   _Read_ and _write_ data from hardware devices.

#### Arithmetic and logic operations\[[edit](https://en.wikipedia.org/w/index.php?title=Instruction_set_architecture&action=edit&section=6 "Edit section: Arithmetic and logic operations")\]

-   _Add_, _subtract_, _multiply_, or _divide_ the values of two registers, placing the result in a register, possibly setting one or more [condition codes](https://en.wikipedia.org/wiki/Flag_(computing) "Flag (computing)") in a [status register](https://en.wikipedia.org/wiki/Status_register "Status register").<sup id="cite_ref-FOOTNOTEHennessyPatterson2003108_6-0"><a href="https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_note-FOOTNOTEHennessyPatterson2003108-6">[6]</a></sup>
    -   _increment_, _decrement_ in some ISAs, saving operand fetch in trivial cases.
-   Perform [bitwise operations](https://en.wikipedia.org/wiki/Bitwise_operation "Bitwise operation"), e.g., taking the _[conjunction](https://en.wikipedia.org/wiki/Logical_conjunction "Logical conjunction")_ and _[disjunction](https://en.wikipedia.org/wiki/Logical_disjunction "Logical disjunction")_ of corresponding bits in a pair of registers, taking the _[negation](https://en.wikipedia.org/wiki/Logical_negation "Logical negation")_ of each bit in a register.
-   _Compare_ two values in registers (for example, to see if one is less, or if they are equal).
-   _Floating-point instructions_ for arithmetic on floating-point numbers.<sup id="cite_ref-FOOTNOTEHennessyPatterson2003108_6-1"><a href="https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_note-FOOTNOTEHennessyPatterson2003108-6">[6]</a></sup>

#### Control flow operations\[[edit](https://en.wikipedia.org/w/index.php?title=Instruction_set_architecture&action=edit&section=7 "Edit section: Control flow operations")\]

-   _[Branch](https://en.wikipedia.org/wiki/Branch_(computer_science) "Branch (computer science)")_ to another location in the program and execute instructions there.
-   _[Conditionally branch](https://en.wikipedia.org/wiki/Branch_predication "Branch predication")_ to another location if a certain condition holds.
-   _[Indirectly branch](https://en.wikipedia.org/wiki/Indirect_branch "Indirect branch")_ to another location.
-   _[Call](https://en.wikipedia.org/wiki/Subroutine "Subroutine")_ another block of code, while saving the location of the next instruction as a point to return to.

#### Coprocessor instructions\[[edit](https://en.wikipedia.org/w/index.php?title=Instruction_set_architecture&action=edit&section=8 "Edit section: Coprocessor instructions")\]

-   Load/store data to and from a coprocessor or exchanging with CPU registers.
-   Perform coprocessor operations.

### Complex instructions\[[edit](https://en.wikipedia.org/w/index.php?title=Instruction_set_architecture&action=edit&section=9 "Edit section: Complex instructions")\]

Processors may include "complex" instructions in their instruction set. A single "complex" instruction does something that may take many instructions on other computers. Such instructions are [typified](https://en.wikipedia.org/wiki/Typified "Typified") by instructions that take multiple steps, control multiple functional units, or otherwise appear on a larger scale than the bulk of simple instructions implemented by the given processor. Some examples of "complex" instructions include:

-   transferring multiple registers to or from memory (especially the [stack](https://en.wikipedia.org/wiki/Call_stack "Call stack")) at once
-   moving large blocks of memory (e.g. [string copy](https://en.wikipedia.org/wiki/String_copy "String copy") or [DMA transfer](https://en.wikipedia.org/wiki/DMA_transfer "DMA transfer"))
-   complicated integer and [floating-point arithmetic](https://en.wikipedia.org/wiki/Floating-point_arithmetic "Floating-point arithmetic") (e.g. [square root](https://en.wikipedia.org/wiki/Square_root "Square root"), or [transcendental functions](https://en.wikipedia.org/wiki/Transcendental_function "Transcendental function") such as [logarithm](https://en.wikipedia.org/wiki/Logarithm "Logarithm"), [sine](https://en.wikipedia.org/wiki/Sine "Sine"), [cosine](https://en.wikipedia.org/wiki/Cosine "Cosine"), etc.)
-   _[SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data "Single instruction, multiple data") instructions_, a single instruction performing an operation on many homogeneous values in parallel, possibly in dedicated [SIMD registers](https://en.wikipedia.org/wiki/SIMD_register "SIMD register")
-   performing an atomic [test-and-set](https://en.wikipedia.org/wiki/Test-and-set "Test-and-set") instruction or other [read-modify-write](https://en.wikipedia.org/wiki/Read-modify-write "Read-modify-write") [atomic instruction](https://en.wikipedia.org/wiki/Atomic_instruction "Atomic instruction")
-   instructions that perform [ALU](https://en.wikipedia.org/wiki/Arithmetic_logic_unit "Arithmetic logic unit") operations with an operand from memory rather than a register

Complex instructions are more common in CISC instruction sets than in RISC instruction sets, but RISC instruction sets may include them as well. RISC instruction sets generally do not include ALU operations with memory operands, or instructions to move large blocks of memory, but most RISC instruction sets include [SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data "Single instruction, multiple data") or [vector](https://en.wikipedia.org/wiki/Vector_processing "Vector processing") instructions that perform the same arithmetic operation on multiple pieces of data at the same time. SIMD instructions have the ability of manipulating large vectors and matrices in minimal time. SIMD instructions allow easy [parallelization](https://en.wikipedia.org/wiki/Parallelization "Parallelization") of algorithms commonly involved in sound, image, and video processing. Various SIMD implementations have been brought to market under trade names such as [MMX](https://en.wikipedia.org/wiki/MMX_(instruction_set) "MMX (instruction set)"), [3DNow!](https://en.wikipedia.org/wiki/3DNow! "3DNow!"), and [AltiVec](https://en.wikipedia.org/wiki/AltiVec "AltiVec").

### Instruction encoding\[[edit](https://en.wikipedia.org/w/index.php?title=Instruction_set_architecture&action=edit&section=10 "Edit section: Instruction encoding")\]

[![](https://upload.wikimedia.org/wikipedia/commons/thumb/2/2a/Mips32_addi.svg/370px-Mips32_addi.svg.png)](https://en.wikipedia.org/wiki/File:Mips32_addi.svg)

One instruction may have several fields, which identify the logical operation, and may also include source and destination addresses and constant values. This is the MIPS "Add Immediate" instruction, which allows selection of source and destination registers and inclusion of a small constant.

On traditional architectures, an instruction includes an [opcode](https://en.wikipedia.org/wiki/Opcode "Opcode") that specifies the operation to perform, such as _add contents of memory to register_—and zero or more [operand](https://en.wikipedia.org/wiki/Operand "Operand") specifiers, which may specify [registers](https://en.wikipedia.org/wiki/Processor_register "Processor register"), memory locations, or literal data. The operand specifiers may have [addressing modes](https://en.wikipedia.org/wiki/Addressing_mode "Addressing mode") determining their meaning or may be in fixed fields. In [very long instruction word](https://en.wikipedia.org/wiki/Very_long_instruction_word "Very long instruction word") (VLIW) architectures, which include many [microcode](https://en.wikipedia.org/wiki/Microcode "Microcode") architectures, multiple simultaneous opcodes and operands are specified in a single instruction.

Some exotic instruction sets do not have an opcode field, such as [transport triggered architectures](https://en.wikipedia.org/wiki/Transport_triggered_architecture "Transport triggered architecture") (TTA), only operand(s).

Most [stack machines](https://en.wikipedia.org/wiki/Stack_machine "Stack machine") have "[0-operand](https://en.wikipedia.org/wiki/0-operand_instruction_set "0-operand instruction set")" instruction sets in which arithmetic and logical operations lack any operand specifier fields; only instructions that push operands onto the evaluation stack or that pop operands from the stack into variables have operand specifiers. The instruction set carries out most ALU actions with postfix ([reverse Polish notation](https://en.wikipedia.org/wiki/Reverse_Polish_notation "Reverse Polish notation")) operations that work only on the expression [stack](https://en.wikipedia.org/wiki/Stack_(abstract_data_type) "Stack (abstract data type)"), not on data registers or arbitrary main memory cells. This can be very convenient for compiling high-level languages, because most arithmetic expressions can be easily translated into postfix notation.<sup id="cite_ref-7"><a href="https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_note-7">[7]</a></sup>

Conditional instructions often have a predicate field—a few bits that encode the specific condition to cause an operation to be performed rather than not performed. For example, a conditional branch instruction will transfer control if the condition is true, so that execution proceeds to a different part of the program, and not transfer control if the condition is false, so that execution continues sequentially. Some instruction sets also have conditional moves, so that the move will be executed, and the data stored in the target location, if the condition is true, and not executed, and the target location not modified, if the condition is false. Similarly, IBM [z/Architecture](https://en.wikipedia.org/wiki/Z/Architecture "Z/Architecture") has a conditional store instruction. A few instruction sets include a predicate field in every instruction; this is called [branch predication](https://en.wikipedia.org/wiki/Branch_predication "Branch predication").

#### Number of operands\[[edit](https://en.wikipedia.org/w/index.php?title=Instruction_set_architecture&action=edit&section=11 "Edit section: Number of operands")\]

Instruction sets may be categorized by the maximum number of operands _explicitly_ specified in instructions.

(In the examples that follow, _a_, _b_, and _c_ are (direct or calculated) addresses referring to memory cells, while _reg1_ and so on refer to machine registers.)

```
C = A+B

```

-   0-operand (_zero-address machines_), so called [stack machines](https://en.wikipedia.org/wiki/Stack_machine "Stack machine"): All arithmetic operations take place using the top one or two positions on the stack:<sup id="cite_ref-FOOTNOTEHennessyPatterson200392_8-0"><a href="https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_note-FOOTNOTEHennessyPatterson200392-8">[8]</a></sup> `push a`, `push b`, `add`, `pop c`.
    -   `C = A+B` needs _four instructions_.<sup id="cite_ref-FOOTNOTEHennessyPatterson200393_9-0"><a href="https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_note-FOOTNOTEHennessyPatterson200393-9">[9]</a></sup> For stack machines, the terms "0-operand" and "zero-address" apply to arithmetic instructions, but not to all instructions, as 1-operand push and pop instructions are used to access memory.
-   1-operand (_one-address machines_), so called [accumulator machines](https://en.wikipedia.org/wiki/Accumulator_machine "Accumulator machine"), include early computers and many small [microcontrollers](https://en.wikipedia.org/wiki/Microcontroller "Microcontroller"): most instructions specify a single right operand (that is, constant, a register, or a memory location), with the implicit [accumulator](https://en.wikipedia.org/wiki/Accumulator_(computing) "Accumulator (computing)") as the left operand (and the destination if there is one): `load a`, `add b`, `store c`.
    -   `C = A+B` needs _three instructions_.<sup id="cite_ref-FOOTNOTEHennessyPatterson200393_9-1"><a href="https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_note-FOOTNOTEHennessyPatterson200393-9">[9]</a></sup>
-   2-operand — many CISC and RISC machines fall under this category:
    -   CISC — `move A` to _C_; then `add B` to _C_.
        -   `C = A+B` needs _two instructions_. This effectively 'stores' the result without an explicit _store_ instruction.
    -   CISC — Often machines are [limited to one memory operand](https://web.archive.org/web/20131105155703/http://cs.smith.edu/~thiebaut/ArtOfAssembly/CH04/CH04-3.html#HEADING3-79) per instruction: `load a,reg1`; `add b,reg1`; `store reg1,c`; This requires a load/store pair for any memory movement regardless of whether the `add` result is an augmentation stored to a different place, as in `C = A+B`, or the same memory location: `A = A+B`.
        -   `C = A+B` needs _three instructions_.
    -   RISC — Requiring explicit memory loads, the instructions would be: `load a,reg1`; `load b,reg2`; `add reg1,reg2`; `store reg2,c`.
        -   `C = A+B` needs _four instructions_.
-   3-operand, allowing better reuse of data:<sup id="cite_ref-Cocke_10-0"><a href="https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_note-Cocke-10">[10]</a></sup>
    -   CISC — It becomes either a single instruction: `add a,b,c`
        -   `C = A+B` needs _one instruction_.
    -   CISC — Or, on machines limited to two memory operands per instruction, `move a,reg1`; `add reg1,b,c`;
        -   `C = A+B` needs _two instructions_.
    -   RISC — arithmetic instructions use registers only, so explicit 2-operand load/store instructions are needed: `load a,reg1`; `load b,reg2`; `add reg1+reg2->reg3`; `store reg3,c`;
        -   `C = A+B` needs _four instructions_.
        -   Unlike 2-operand or 1-operand, this leaves all three values a, b, and c in registers available for further reuse.<sup id="cite_ref-Cocke_10-1"><a href="https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_note-Cocke-10">[10]</a></sup>
-   more operands—some CISC machines permit a variety of addressing modes that allow more than 3 operands (registers or memory accesses), such as the [VAX](https://en.wikipedia.org/wiki/VAX "VAX") "POLY" polynomial evaluation instruction.

Due to the large number of bits needed to encode the three registers of a 3-operand instruction, RISC architectures that have 16-bit instructions are invariably 2-operand designs, such as the Atmel AVR, [TI MSP430](https://en.wikipedia.org/wiki/TI_MSP430 "TI MSP430"), and some versions of [ARM Thumb](https://en.wikipedia.org/wiki/ARM_Thumb "ARM Thumb"). RISC architectures that have 32-bit instructions are usually 3-operand designs, such as the [ARM](https://en.wikipedia.org/wiki/ARM_architecture "ARM architecture"), [AVR32](https://en.wikipedia.org/wiki/AVR32 "AVR32"), [MIPS](https://en.wikipedia.org/wiki/MIPS_architecture "MIPS architecture"), [Power ISA](https://en.wikipedia.org/wiki/Power_ISA "Power ISA"), and [SPARC](https://en.wikipedia.org/wiki/SPARC "SPARC") architectures.

Each instruction specifies some number of operands (registers, memory locations, or immediate values) _explicitly_. Some instructions give one or both operands implicitly, such as by being stored on top of the [stack](https://en.wikipedia.org/wiki/Stack_(data_structure) "Stack (data structure)") or in an implicit register. If some of the operands are given implicitly, fewer operands need be specified in the instruction. When a "destination operand" explicitly specifies the destination, an additional operand must be supplied. Consequently, the number of operands encoded in an instruction may differ from the mathematically necessary number of arguments for a logical or arithmetic operation (the [arity](https://en.wikipedia.org/wiki/Arity "Arity")). Operands are either encoded in the "opcode" representation of the instruction, or else are given as values or addresses following the opcode.

### Register pressure\[[edit](https://en.wikipedia.org/w/index.php?title=Instruction_set_architecture&action=edit&section=12 "Edit section: Register pressure")\]

_Register pressure_ measures the availability of free registers at any point in time during the program execution. Register pressure is high when a large number of the available registers are in use; thus, the higher the register pressure, the more often the register contents must be [spilled](https://en.wikipedia.org/wiki/Register_spilling "Register spilling") into memory. Increasing the number of registers in an architecture decreases register pressure but increases the cost.<sup id="cite_ref-11"><a href="https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_note-11">[11]</a></sup>

While embedded instruction sets such as [Thumb](https://en.wikipedia.org/wiki/ARM_Thumb "ARM Thumb") suffer from extremely high register pressure because they have small register sets, general-purpose RISC ISAs like [MIPS](https://en.wikipedia.org/wiki/MIPS_architecture "MIPS architecture") and [Alpha](https://en.wikipedia.org/wiki/DEC_Alpha "DEC Alpha") enjoy low register pressure. CISC ISAs like x86-64 offer low register pressure despite having smaller register sets. This is due to the many addressing modes and optimizations (such as sub-register addressing, memory operands in ALU instructions, absolute addressing, PC-relative addressing, and register-to-register spills) that CISC ISAs offer.<sup id="cite_ref-12"><a href="https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_note-12">[12]</a></sup>

### Instruction length\[[edit](https://en.wikipedia.org/w/index.php?title=Instruction_set_architecture&action=edit&section=13 "Edit section: Instruction length")\]

The size or length of an instruction varies widely, from as little as four bits in some [microcontrollers](https://en.wikipedia.org/wiki/Microcontroller "Microcontroller") to many hundreds of bits in some [VLIW](https://en.wikipedia.org/wiki/Very_long_instruction_word "Very long instruction word") systems. Processors used in [personal computers](https://en.wikipedia.org/wiki/Personal_computer "Personal computer"), [mainframes](https://en.wikipedia.org/wiki/Mainframe_computer "Mainframe computer"), and [supercomputers](https://en.wikipedia.org/wiki/Supercomputer "Supercomputer") have minimum instruction sizes between 8 and 64 bits. The longest possible instruction on x86 is 15 bytes (120 bits).<sup id="cite_ref-13"><a href="https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_note-13">[13]</a></sup> Within an instruction set, different instructions may have different lengths. In some architectures, notably most [reduced instruction set computers](https://en.wikipedia.org/wiki/Reduced_instruction_set_computer "Reduced instruction set computer") (RISC), instructions are a fixed length, typically corresponding with that architecture's [word size](https://en.wikipedia.org/wiki/Word_(data_type) "Word (data type)"). In other architectures, instructions have [variable length](https://en.wikipedia.org/wiki/Variable-length_code "Variable-length code"), typically integral multiples of a [byte](https://en.wikipedia.org/wiki/Byte "Byte") or a [halfword](https://en.wikipedia.org/wiki/Halfword "Halfword"). Some, such as the [ARM](https://en.wikipedia.org/wiki/ARMv7 "ARMv7") with _Thumb-extension_ have _mixed_ variable encoding, that is two fixed, usually 32-bit and 16-bit encodings, where instructions cannot be mixed freely but must be switched between on a branch (or exception boundary in ARMv8).

Fixed-length instructions are less complicated to handle than variable-length instructions for several reasons (not having to check whether an instruction straddles a cache line or virtual memory page boundary,<sup id="cite_ref-Cocke_10-2"><a href="https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_note-Cocke-10">[10]</a></sup> for instance), and are therefore somewhat easier to optimize for speed.

### Code density\[[edit](https://en.wikipedia.org/w/index.php?title=Instruction_set_architecture&action=edit&section=14 "Edit section: Code density")\]

In early 1960s computers, main memory was expensive and very limited, even on mainframes. Minimizing the size of a program to make sure it would fit in the limited memory was often central. Thus the size of the instructions needed to perform a particular task, the _code density_, was an important characteristic of any instruction set. It remained important on the initially-tiny memories of minicomputers and then microprocessors. Density remains important today, for smartphone applications, applications downloaded into browsers over slow Internet connections, and in ROMs for embedded applications. A more general advantage of increased density is improved effectiveness of caches and instruction prefetch.

Computers with high code density often have complex instructions for procedure entry, parameterized returns, loops, etc. (therefore retroactively named _Complex Instruction Set Computers_, [CISC](https://en.wikipedia.org/wiki/Complex_instruction_set_computer "Complex instruction set computer")). However, more typical, or frequent, "CISC" instructions merely combine a basic ALU operation, such as "add", with the access of one or more operands in memory (using [addressing modes](https://en.wikipedia.org/wiki/Addressing_mode "Addressing mode") such as direct, indirect, indexed, etc.). Certain architectures may allow two or three operands (including the result) directly in memory or may be able to perform functions such as automatic pointer increment, etc. Software-implemented instruction sets may have even more complex and powerful instructions.

_Reduced instruction-set computers_, [RISC](https://en.wikipedia.org/wiki/Reduced_instruction_set_computer "Reduced instruction set computer"), were first widely implemented during a period of rapidly growing memory subsystems. They sacrifice code density to simplify implementation circuitry, and try to increase performance via higher clock frequencies and more registers. A single RISC instruction typically performs only a single operation, such as an "add" of registers or a "load" from a memory location into a register. A RISC instruction set normally has a fixed [instruction length](https://en.wikipedia.org/wiki/Instruction_set_architecture#Instruction_length), whereas a typical CISC instruction set has instructions of widely varying length. However, as RISC computers normally require more and often longer instructions to implement a given task, they inherently make less optimal use of bus bandwidth and cache memories.

Certain embedded RISC ISAs like [Thumb](https://en.wikipedia.org/wiki/ARM_architecture#Thumb "ARM architecture") and [AVR32](https://en.wikipedia.org/wiki/AVR32 "AVR32") typically exhibit very high density owing to a technique called code compression. This technique packs two 16-bit instructions into one 32-bit word, which is then unpacked at the decode stage and executed as two instructions.<sup id="cite_ref-weaver_14-0"><a href="https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_note-weaver-14">[14]</a></sup>

[Minimal instruction set computers](https://en.wikipedia.org/wiki/Minimal_instruction_set_computer "Minimal instruction set computer") (MISC) are commonly a form of [stack machine](https://en.wikipedia.org/wiki/Stack_machine "Stack machine"), where there are few separate instructions (8–32), so that multiple instructions can be fit into a single machine word. These types of cores often take little silicon to implement, so they can be easily realized in an [FPGA](https://en.wikipedia.org/wiki/Field-programmable_gate_array "Field-programmable gate array") or in a [multi-core](https://en.wikipedia.org/wiki/Multi-core "Multi-core") form. The code density of MISC is similar to the code density of RISC; the increased instruction density is offset by requiring more of the primitive instructions to do a task.<sup id="cite_ref-15"><a href="https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_note-15">[15]</a></sup><sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Verifiability" title="Wikipedia:Verifiability"><span title="That discusses RISC and CISC, but not MISC. (December 2021)">failed verification</span></a></i>]</sup>

There has been research into [executable compression](https://en.wikipedia.org/wiki/Executable_compression "Executable compression") as a mechanism for improving code density. The mathematics of [Kolmogorov complexity](https://en.wikipedia.org/wiki/Kolmogorov_complexity "Kolmogorov complexity") describes the challenges and limits of this.

In practice, code density is also dependent on the [compiler](https://en.wikipedia.org/wiki/Compiler "Compiler"). Most [optimizing compilers](https://en.wikipedia.org/wiki/Optimizing_compilers "Optimizing compilers") have options that control whether to optimize code generation for execution speed or for code density. For instance [GCC](https://en.wikipedia.org/wiki/GNU_Compiler_Collection "GNU Compiler Collection") has the option -Os to optimize for small machine code size, and -O3 to optimize for execution speed at the cost of larger machine code.

### Representation\[[edit](https://en.wikipedia.org/w/index.php?title=Instruction_set_architecture&action=edit&section=15 "Edit section: Representation")\]

The instructions constituting a program are rarely specified using their internal, numeric form ([machine code](https://en.wikipedia.org/wiki/Machine_code "Machine code")); they may be specified by programmers using an [assembly language](https://en.wikipedia.org/wiki/Assembly_language "Assembly language") or, more commonly, may be generated from [high-level programming languages](https://en.wikipedia.org/wiki/High-level_programming_language "High-level programming language") by [compilers](https://en.wikipedia.org/wiki/Compiler "Compiler").<sup id="cite_ref-FOOTNOTEHennessyPatterson2003120_16-0"><a href="https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_note-FOOTNOTEHennessyPatterson2003120-16">[16]</a></sup>

## Design\[[edit](https://en.wikipedia.org/w/index.php?title=Instruction_set_architecture&action=edit&section=16 "Edit section: Design")\]

The design of instruction sets is a complex issue. There were two stages in history for the microprocessor. The first was the CISC (Complex Instruction Set Computer), which had many different instructions. In the 1970s, however, places like IBM did research and found that many instructions in the set could be eliminated. The result was the RISC (Reduced Instruction Set Computer), an architecture that uses a smaller set of instructions. A simpler instruction set may offer the potential for higher speeds, reduced processor size, and reduced power consumption. However, a more complex set may optimize common operations, improve memory and [cache](https://en.wikipedia.org/wiki/CPU_cache "CPU cache") efficiency, or simplify programming.

Some instruction set designers reserve one or more opcodes for some kind of [system call](https://en.wikipedia.org/wiki/System_call "System call") or [software interrupt](https://en.wikipedia.org/wiki/Software_interrupt "Software interrupt"). For example, [MOS Technology 6502](https://en.wikipedia.org/wiki/MOS_Technology_6502 "MOS Technology 6502") uses 00<sub>H</sub>, [Zilog Z80](https://en.wikipedia.org/wiki/Zilog_Z80 "Zilog Z80") uses the eight codes C7,CF,D7,DF,E7,EF,F7,FF<sub>H</sub><sup id="cite_ref-17"><a href="https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_note-17">[17]</a></sup> while [Motorola 68000](https://en.wikipedia.org/wiki/Motorola_68000 "Motorola 68000") use codes in the range A000..AFFF<sub>H</sub>.

Fast virtual machines are much easier to implement if an instruction set meets the [Popek and Goldberg virtualization requirements](https://en.wikipedia.org/wiki/Popek_and_Goldberg_virtualization_requirements "Popek and Goldberg virtualization requirements").<sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="The text near this tag may need clarification or removal of jargon. (October 2012)">clarification needed</span></a></i>]</sup>

The [NOP slide](https://en.wikipedia.org/wiki/NOP_slide "NOP slide") used in immunity-aware programming is much easier to implement if the "unprogrammed" state of the memory is interpreted as a [NOP](https://en.wikipedia.org/wiki/NOP_(code) "NOP (code)").<sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Accuracy_dispute#Disputed_statement" title="Wikipedia:Accuracy dispute"><span title="The material near this tag is possibly inaccurate or nonfactual. (October 2012)">dubious</span></a> <span>– <a href="https://en.wikipedia.org/wiki/Talk:Instruction_set_architecture#Dubious" title="Talk:Instruction set architecture">discuss</a></span></i>]</sup>

On systems with multiple processors, [non-blocking synchronization](https://en.wikipedia.org/wiki/Non-blocking_synchronization "Non-blocking synchronization") algorithms are much easier to implement<sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="Opinion unsupported by a textbook (October 2012)">citation needed</span></a></i>]</sup> if the instruction set includes support for something such as "[fetch-and-add](https://en.wikipedia.org/wiki/Fetch-and-add "Fetch-and-add")", "[load-link/store-conditional](https://en.wikipedia.org/wiki/Load-link/store-conditional "Load-link/store-conditional")" (LL/SC), or "atomic [compare-and-swap](https://en.wikipedia.org/wiki/Compare-and-swap "Compare-and-swap")".

## Instruction set implementation\[[edit](https://en.wikipedia.org/w/index.php?title=Instruction_set_architecture&action=edit&section=17 "Edit section: Instruction set implementation")\]

Any given instruction set can be implemented in a variety of ways. All ways of implementing a particular instruction set provide the same [programming model](https://en.wikipedia.org/wiki/Programming_model "Programming model"), and all implementations of that instruction set are able to run the same executables. The various ways of implementing an instruction set give different tradeoffs between cost, performance, power consumption, size, etc.

When designing the [microarchitecture](https://en.wikipedia.org/wiki/Microarchitecture "Microarchitecture") of a processor, engineers use blocks of "hard-wired" electronic circuitry (often designed separately) such as adders, multiplexers, counters, registers, ALUs, etc. Some kind of [register transfer language](https://en.wikipedia.org/wiki/Register_transfer_language "Register transfer language") is then often used to describe the decoding and sequencing of each instruction of an ISA using this physical microarchitecture. There are two basic ways to build a [control unit](https://en.wikipedia.org/wiki/Control_unit "Control unit") to implement this description (although many designs use middle ways or compromises):

1.  Some computer designs "hardwire" the complete instruction set decoding and sequencing (just like the rest of the microarchitecture).
2.  Other designs employ [microcode](https://en.wikipedia.org/wiki/Microcode "Microcode") routines or tables (or both) to do this—typically as on-chip [ROMs](https://en.wikipedia.org/wiki/Read-only_memory "Read-only memory") or [PLAs](https://en.wikipedia.org/wiki/Programmable_logic_array "Programmable logic array") or both (although separate RAMs and [ROMs](https://en.wikipedia.org/wiki/Read-only_memory#Historical_examples "Read-only memory") have been used historically). The [Western Digital](https://en.wikipedia.org/wiki/Western_Digital "Western Digital") [MCP-1600](https://en.wikipedia.org/wiki/MCP-1600 "MCP-1600") is an older example, using a dedicated, separate ROM for microcode.

Some designs use a combination of hardwired design and microcode for the control unit.

Some CPU designs use a [writable control store](https://en.wikipedia.org/wiki/Writable_control_store "Writable control store")—they compile the instruction set to a writable [RAM](https://en.wikipedia.org/wiki/RAM "RAM") or [flash](https://en.wikipedia.org/wiki/Flash_memory "Flash memory") inside the CPU (such as the [Rekursiv](https://en.wikipedia.org/wiki/Rekursiv "Rekursiv") processor and the [Imsys](https://en.wikipedia.org/w/index.php?title=Imsys&action=edit&redlink=1 "Imsys (page does not exist)") [Cjip](https://en.wikipedia.org/w/index.php?title=Cjip&action=edit&redlink=1 "Cjip (page does not exist)")),<sup id="cite_ref-18"><a href="https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_note-18">[18]</a></sup> or an FPGA ([reconfigurable computing](https://en.wikipedia.org/wiki/Reconfigurable_computing "Reconfigurable computing")).

An ISA can also be [emulated](https://en.wikipedia.org/wiki/Emulator "Emulator") in software by an [interpreter](https://en.wikipedia.org/wiki/Interpreter_(computing) "Interpreter (computing)"). Naturally, due to the interpretation overhead, this is slower than directly running programs on the emulated hardware, unless the hardware running the emulator is an order of magnitude faster. Today, it is common practice for vendors of new ISAs or microarchitectures to make software emulators available to software developers before the hardware implementation is ready.

Often the details of the implementation have a strong influence on the particular instructions selected for the instruction set. For example, many implementations of the [instruction pipeline](https://en.wikipedia.org/wiki/Instruction_pipeline "Instruction pipeline") only allow a single memory load or memory store per instruction, leading to a [load–store architecture](https://en.wikipedia.org/wiki/Load%E2%80%93store_architecture "Load–store architecture") (RISC). For another example, some early ways of implementing the [instruction pipeline](https://en.wikipedia.org/wiki/Instruction_pipeline "Instruction pipeline") led to a [delay slot](https://en.wikipedia.org/wiki/Delay_slot "Delay slot").

The demands of high-speed digital signal processing have pushed in the opposite direction—forcing instructions to be implemented in a particular way. For example, to perform digital filters fast enough, the MAC instruction in a typical [digital signal processor](https://en.wikipedia.org/wiki/Digital_signal_processor "Digital signal processor") (DSP) must use a kind of [Harvard architecture](https://en.wikipedia.org/wiki/Harvard_architecture "Harvard architecture") that can fetch an instruction and two data words simultaneously, and it requires a single-cycle [multiply–accumulate](https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate "Multiply–accumulate") [multiplier](https://en.wikipedia.org/wiki/Binary_multiplier "Binary multiplier").

## See also\[[edit](https://en.wikipedia.org/w/index.php?title=Instruction_set_architecture&action=edit&section=18 "Edit section: See also")\]

-   [Comparison of instruction set architectures](https://en.wikipedia.org/wiki/Comparison_of_instruction_set_architectures "Comparison of instruction set architectures")
-   [Computer architecture](https://en.wikipedia.org/wiki/Computer_architecture "Computer architecture")
-   [Processor design](https://en.wikipedia.org/wiki/Processor_design "Processor design")
-   [Compressed instruction set](https://en.wikipedia.org/wiki/Compressed_instruction_set "Compressed instruction set")
-   [Emulator](https://en.wikipedia.org/wiki/Emulator "Emulator")
-   [Simulation](https://en.wikipedia.org/wiki/Simulation "Simulation")
-   [Instruction set simulator](https://en.wikipedia.org/wiki/Instruction_set_simulator "Instruction set simulator")
-   [OVPsim](https://en.wikipedia.org/wiki/OVPsim "OVPsim") full systems simulator providing ability to create/model/emulate any instruction set using C and standard APIs
-   [Register transfer language](https://en.wikipedia.org/wiki/Register_transfer_language "Register transfer language") (RTL)
-   [Micro-operation](https://en.wikipedia.org/wiki/Micro-operation "Micro-operation")

## References\[[edit](https://en.wikipedia.org/w/index.php?title=Instruction_set_architecture&action=edit&section=19 "Edit section: References")\]

1.  **[^](https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_ref-Pugh_1-0 "Jump up")** Pugh, Emerson W.; Johnson, Lyle R.; Palmer, John H. (1991). [_IBM's 360 and Early 370 Systems_](https://archive.org/details/ibms360early370s0000pugh). MIT Press. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [0-262-16123-0](https://en.wikipedia.org/wiki/Special:BookSources/0-262-16123-0 "Special:BookSources/0-262-16123-0").
2.  **[^](https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_ref-2 "Jump up")** Crystal Chen; Greg Novick; Kirk Shimano (December 16, 2006). ["RISC Architecture: RISC vs. CISC"](http://cs.stanford.edu/people/eroberts/courses/soco/projects/risc/risccisc/). _cs.stanford.edu_. Retrieved February 21, 2015.
3.  **[^](https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_ref-3 "Jump up")** Schlansker, Michael S.; Rau, B. Ramakrishna (February 2000). "EPIC: Explicitly Parallel Instruction Computing". _[Computer](https://en.wikipedia.org/wiki/Computer_(magazine) "Computer (magazine)")_. **33** (2). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1109/2.820037](https://doi.org/10.1109%2F2.820037).
4.  **[^](https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_ref-4 "Jump up")** Shaout, Adnan; Eldos, Taisir (Summer 2003). ["On the Classification of Computer Architecture"](https://www.researchgate.net/publication/267239549_On_the_Classification_of_Computer_Architecture). _International Journal of Science and Technology_. **14**: 3. Retrieved March 2, 2023.
5.  **[^](https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_ref-5 "Jump up")** Gilreath, William F.; Laplante, Phillip A. (December 6, 2012). _Computer Architecture: A Minimalist Perspective_. [Springer Science+Business Media](https://en.wikipedia.org/wiki/Springer_Science%2BBusiness_Media "Springer Science+Business Media"). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-1-4615-0237-1](https://en.wikipedia.org/wiki/Special:BookSources/978-1-4615-0237-1 "Special:BookSources/978-1-4615-0237-1").
6.  ^ [Jump up to: <sup><i><b>a</b></i></sup>](https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_ref-FOOTNOTEHennessyPatterson2003108_6-0) [<sup><i><b>b</b></i></sup>](https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_ref-FOOTNOTEHennessyPatterson2003108_6-1) [Hennessy & Patterson 2003](https://en.wikipedia.org/wiki/Instruction_set_architecture#CITEREFHennessyPatterson2003), p. 108.
7.  **[^](https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_ref-7 "Jump up")** Durand, Paul. ["Instruction Set Architecture (ISA)"](http://www.cs.kent.edu/~durand/CS0/Notes/Chapter05/isa.html). _Introduction to Computer Science CS 0_.
8.  **[^](https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_ref-FOOTNOTEHennessyPatterson200392_8-0 "Jump up")** [Hennessy & Patterson 2003](https://en.wikipedia.org/wiki/Instruction_set_architecture#CITEREFHennessyPatterson2003), p. 92.
9.  ^ [Jump up to: <sup><i><b>a</b></i></sup>](https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_ref-FOOTNOTEHennessyPatterson200393_9-0) [<sup><i><b>b</b></i></sup>](https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_ref-FOOTNOTEHennessyPatterson200393_9-1) [Hennessy & Patterson 2003](https://en.wikipedia.org/wiki/Instruction_set_architecture#CITEREFHennessyPatterson2003), p. 93.
10.  ^ [Jump up to: <sup><i><b>a</b></i></sup>](https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_ref-Cocke_10-0) [<sup><i><b>b</b></i></sup>](https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_ref-Cocke_10-1) [<sup><i><b>c</b></i></sup>](https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_ref-Cocke_10-2) Cocke, John; Markstein, Victoria (January 1990). ["The evolution of RISC technology at IBM"](https://www.cis.upenn.edu/~milom/cis501-Fall11/papers/cocke-RISC.pdf) (PDF). _IBM Journal of Research and Development_. **34** (1): 4–11. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1147/rd.341.0004](https://doi.org/10.1147%2Frd.341.0004). Retrieved 2022-10-05.
11.  **[^](https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_ref-11 "Jump up")** Page, Daniel (2009). "11. Compilers". _A Practical Introduction to Computer Architecture_. Springer. p. 464. [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) "Bibcode (identifier)"):[2009pica.book.....P](https://ui.adsabs.harvard.edu/abs/2009pica.book.....P). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-1-84882-255-9](https://en.wikipedia.org/wiki/Special:BookSources/978-1-84882-255-9 "Special:BookSources/978-1-84882-255-9").
12.  **[^](https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_ref-12 "Jump up")** Venkat, Ashish; Tullsen, Dean M. (2014). [_Harnessing ISA Diversity: Design of a Heterogeneous-ISA Chip Multiprocessor_](http://dl.acm.org/citation.cfm?id=2665692). 41st Annual International Symposium on Computer Architecture.
13.  **[^](https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_ref-13 "Jump up")** ["Intel® 64 and IA-32 Architectures Software Developer's Manual"](https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html). Intel Corporation. Retrieved 5 October 2022.
14.  **[^](https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_ref-weaver_14-0 "Jump up")** Weaver, Vincent M.; McKee, Sally A. (2009). _Code density concerns for new architectures_. IEEE International Conference on Computer Design. [CiteSeerX](https://en.wikipedia.org/wiki/CiteSeerX_(identifier) "CiteSeerX (identifier)") [10.1.1.398.1967](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.398.1967). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1109/ICCD.2009.5413117](https://doi.org/10.1109%2FICCD.2009.5413117).
15.  **[^](https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_ref-15 "Jump up")** ["RISC vs. CISC"](https://cs.stanford.edu/people/eroberts/courses/soco/projects/risc/risccisc/). _cs.stanford.edu_. Retrieved 2021-12-18.
16.  **[^](https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_ref-FOOTNOTEHennessyPatterson2003120_16-0 "Jump up")** [Hennessy & Patterson 2003](https://en.wikipedia.org/wiki/Instruction_set_architecture#CITEREFHennessyPatterson2003), p. 120.
17.  **[^](https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_ref-17 "Jump up")** Ganssle, Jack (February 26, 2001). ["Proactive Debugging"](https://www.embedded.com/electronics-blogs/break-points/4023293/Proactive-Debugging). _embedded.com_.
18.  **[^](https://en.wikipedia.org/wiki/Instruction_set_architecture#cite_ref-18 "Jump up")** ["Great Microprocessors of the Past and Present (V 13.4.0)"](http://cpushack.net/CPU/cpu7.html). _cpushack.net_. Retrieved 2014-07-25.

## Further reading\[[edit](https://en.wikipedia.org/w/index.php?title=Instruction_set_architecture&action=edit&section=20 "Edit section: Further reading")\]

-   [Bowen, Jonathan P.](https://en.wikipedia.org/wiki/Jonathan_Bowen "Jonathan Bowen") (July–August 1985). "Standard Microprocessor Programming Cards". _Microprocessors and Microsystems_. **9** (6): 274–290. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1016/0141-9331(85)90116-4](https://doi.org/10.1016%2F0141-9331%2885%2990116-4).
-   [Hennessy, John L.](https://en.wikipedia.org/wiki/John_L._Hennessy "John L. Hennessy"); [Patterson, David A.](https://en.wikipedia.org/wiki/David_Patterson_(computer_scientist) "David Patterson (computer scientist)") (2003). [_Computer Architecture: A Quantitative Approach_](https://books.google.com/books?id=XX69oNsazH4C) (Third ed.). [Morgan Kaufmann Publishers](https://en.wikipedia.org/wiki/Morgan_Kaufmann_Publishers "Morgan Kaufmann Publishers"). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [1-55860-724-2](https://en.wikipedia.org/wiki/Special:BookSources/1-55860-724-2 "Special:BookSources/1-55860-724-2"). Retrieved 2023-03-04.

## External links\[[edit](https://en.wikipedia.org/w/index.php?title=Instruction_set_architecture&action=edit&section=21 "Edit section: External links")\]

![](https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/Wikibooks-logo-en-noslogan.svg/40px-Wikibooks-logo-en-noslogan.svg.png)

-   [![](https://upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/12px-Commons-logo.svg.png)](https://en.wikipedia.org/wiki/File:Commons-logo.svg) Media related to [Instruction set architectures](https://commons.wikimedia.org/wiki/Category:Instruction_set_architectures "commons:Category:Instruction set architectures") at Wikimedia Commons
-   [Programming Textfiles: Bowen's Instruction Summary Cards](http://www.textfiles.com/programming/CARDS/)
-   [Mark Smotherman's Historical Computer Designs Page](http://www.cs.clemson.edu/~mark/hist.html)
